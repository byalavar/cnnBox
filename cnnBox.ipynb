{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class boxFilters(nn.Module):\n",
    "    def __init__(self,a,b,c,d):\n",
    "        super(boxFilters, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(1.0),requires_grad=True)\n",
    "        #self.a = nn.Parameter(torch.tensor(a))\n",
    "        #self.b = nn.Parameter(torch.tensor(b))\n",
    "        #self.c = nn.Parameter(torch.tensor(c))\n",
    "        #self.d = nn.Parameter(torch.tensor(d))\n",
    "        self.a = int(a)\n",
    "        self.b = int(b)\n",
    "        self.c = int(c)\n",
    "        self.d = int(d)\n",
    "        # self.a.requires_grad = False\n",
    "        # self.b.requires_grad = False\n",
    "        # self.c.requires_grad = False\n",
    "        # self.d.requires_grad = False\n",
    "\n",
    "\n",
    "    \n",
    "    def setAlpha(self,alpha):\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha),requires_grad=True)\n",
    "    \n",
    "    def setABCD(self,a,b,c,d):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "    \n",
    "    def printParams(self):\n",
    "        print(\"A:\",self.a,\"B:\",self.b,\"C:\",self.c,\"D:\",self.d, \"Alpha:\", self.alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def forward(self, integral_image):\n",
    "        \n",
    "    #     H, W = integral_image.shape[-2:]\n",
    "\n",
    "    #     H = H - self.ogFilterSize + 1\n",
    "    #     W = W - self.ogFilterSize + 1\n",
    "    #     output = torch.zeros((integral_image.shape[0],integral_image.shape[1],H,W)).to(self.device)\n",
    "    #     for box_filter in self.box_filters:\n",
    "    #         a, b, c, d = int(box_filter.a), int(box_filter.b), int(box_filter.c), int(box_filter.d)\n",
    "    #         if b >= a and d >= c:  # Check if the box filter is valid\n",
    "    #             # Compute the sum within the box filter for each position in the image\n",
    "\n",
    "    #           if(self.reg):\n",
    "    #             print(\"Regular Conv\")  \n",
    "    #             for i in range(H):\n",
    "    #                 for j in range(W):\n",
    "    #                     # Define the boundaries\n",
    "                        \n",
    "    #                     tlCol = j + a -1\n",
    "    #                     tlRow = i + c -1\n",
    "\n",
    "    #                     trCol = j + b\n",
    "    #                     trRow = i + c -1\n",
    "\n",
    "\n",
    "    #                     blCol = j + a -1\n",
    "    #                     blRow = i + d\n",
    "\n",
    "    #                     brCol = j + b\n",
    "    #                     brRow = i + d\n",
    "\n",
    "    #                     #print(\"tlCol:\",tlCol,\"tlRow:\",tlRow,\"trCol:\",trCol,\"trRow:\",trRow,\"blCol:\",blCol,\"blRow:\",blRow,\"brCol:\",brCol,\"brRow:\",brRow)\n",
    "    #                     # Get the sum within the boundaries and add it to the output\n",
    "\n",
    "    #                     if(tlRow>=0 and tlCol>=0):\n",
    "    #                         tlVal = integral_image[..., tlRow, tlCol]\n",
    "    #                     else:\n",
    "    #                         tlVal = 0\n",
    "                        \n",
    "    #                     if(trRow>=0 and trCol>=0):\n",
    "    #                         trVal = integral_image[..., trRow, trCol]\n",
    "    #                     else:\n",
    "    #                         trVal = 0\n",
    "                        \n",
    "    #                     if(blRow>=0 and blCol>=0):\n",
    "    #                         blVal = integral_image[..., blRow, blCol]\n",
    "    #                     else:\n",
    "    #                         blVal = 0\n",
    "                        \n",
    "    #                     if(brRow>=0 and brCol>=0):\n",
    "    #                         brVal = integral_image[..., brRow, brCol]\n",
    "    #                     else:\n",
    "    #                         brVal = 0\n",
    "\n",
    "                            \n",
    "                        \n",
    "\n",
    "    #                     #print(integral_image[..., tlRow, tlCol])\n",
    "    #                     output[..., i, j] += brVal  + tlVal - trVal - blVal\n",
    "    #           else:\n",
    "    #                 print(\"SIMD Conv\")\n",
    "\n",
    "    #                 # Define the boundaries\n",
    "    #                 row_indices = torch.arange(H).unsqueeze(-1).to(self.device)\n",
    "    #                 col_indices = torch.arange(W).to(self.device)\n",
    "    #                 print(row_indices.shape,col_indices.shape)\n",
    "    #                 tlCol = col_indices + a - 1\n",
    "    #                 tlRow = row_indices + c - 1\n",
    "\n",
    "    #                 trCol = col_indices + b\n",
    "    #                 trRow = tlRow  # trRow and tlRow are the same\n",
    "\n",
    "    #                 blCol = tlCol  # blCol and tlCol are the same\n",
    "    #                 blRow = row_indices + d\n",
    "\n",
    "    #                 brCol = trCol  # brCol and trCol are the same\n",
    "    #                 brRow = blRow  # brRow and blRow are the same\n",
    "\n",
    "    #                 # Get the values at the boundaries\n",
    "                    \n",
    "    #                 tlVal = torch.where((tlRow < 0) | (tlCol < 0), 0, integral_image[..., tlRow, tlCol])\n",
    "    #                 trVal = torch.where((trRow < 0) | (trCol < 0), 0, integral_image[..., trRow, trCol])\n",
    "    #                 blVal = torch.where((blRow < 0) | (blCol < 0), 0, integral_image[..., blRow, blCol])\n",
    "    #                 brVal = torch.where((brRow < 0) | (brCol < 0), 0, integral_image[..., brRow, brCol])\n",
    "\n",
    "    #                 # Compute the output\n",
    "    #                 output.add_(brVal + tlVal - trVal - blVal)  # in-place addition\n",
    "\n",
    "                        \n",
    "            \n",
    "    #             #print(f\"Invalid box filter: a={a}, b={b}, c={c}, d={d}\")\n",
    "    #     return output\n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, integral_image):\n",
    "        \n",
    "    #     H, W = integral_image.shape[-2:]\n",
    "\n",
    "    #     H = H - self.ogFilterSize + 1\n",
    "    #     W = W - self.ogFilterSize + 1\n",
    "    #     #output = torch.zeros((integral_image.shape[0],integral_image.shape[1],H,W)).to(self.device)\n",
    "    #     # Define the boundaries\n",
    "    #     row_indices = torch.arange(H).unsqueeze(-1).unsqueeze(-1).to(self.device)  # shape: (H, 1, 1)\n",
    "    #     col_indices = torch.arange(W).unsqueeze(-1).unsqueeze(0).to(self.device)  # shape: (1, W, 1)\n",
    "    #    # print(row_indices.shape,col_indices.shape)\n",
    "    #     # a, b, c, d should be 1D tensors of shape (num_filters,)\n",
    "    #     # reshape them to (1, 1, num_filters)\n",
    "\n",
    "    #     #box_filters_tensor = torch.tensor([[int(box_filter.a), int(box_filter.b), int(box_filter.c), int(box_filter.d)] for box_filter in self.box_filters]).to(self.device)\n",
    "    #     box_filters_tensor = torch.tensor([[box_filter.a, box_filter.b, box_filter.c, box_filter.d] for box_filter in self.box_filters]).to(self.device)\n",
    "        \n",
    "        \n",
    "\n",
    "    #     a, b, c, d = box_filters_tensor[:, 0], box_filters_tensor[:, 1], box_filters_tensor[:, 2], box_filters_tensor[:, 3]\n",
    "        \n",
    "    #     a, b, c, d = a.unsqueeze(0).unsqueeze(0), b.unsqueeze(0).unsqueeze(0), c.unsqueeze(0).unsqueeze(0), d.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "    #     #box_filters_alpha = torch.tensor([[box_filter.alpha] for box_filter in self.box_filters]).to(self.device)\n",
    "    #     box_filters_alpha = torch.stack([box_filter.alpha for box_filter in self.box_filters]).to(self.device)\n",
    "    #     #print(\"Alpha Check:\",box_filters_alpha,box_filters_alpha1)\n",
    "    #     #print(\"dtype\",box_filters_alpha.dtype)\n",
    "    #     #print(\"CA Shape:\",col_indices.shape,a.shape)\n",
    "    #     tlCol = col_indices + a - 1  # broadcasting happens here\n",
    "    #     tlRow = row_indices + c - 1\n",
    "\n",
    "    #     trCol = col_indices + b\n",
    "    #     trRow = tlRow  # trRow and tlRow are the same\n",
    "\n",
    "    #     blCol = tlCol  # blCol and tlCol are the same\n",
    "    #     blRow = row_indices + d\n",
    "\n",
    "    #     brCol = trCol  # brCol and trCol are the same\n",
    "    #     brRow = blRow  # brRow and blRow are the same\n",
    "    #     #print(tlCol.shape,tlRow.shape,trCol.shape,trRow.shape,blCol.shape,blRow.shape,brCol.shape,brRow.shape)\n",
    "    #     # Get the values at the boundaries\n",
    "    #     tlVal = torch.where((tlRow < 0) | (tlCol < 0), 0, integral_image[..., tlRow, tlCol])\n",
    "    #     trVal = torch.where((trRow < 0) | (trCol < 0), 0, integral_image[..., trRow, trCol])\n",
    "    #     blVal = torch.where((blRow < 0) | (blCol < 0), 0, integral_image[..., blRow, blCol])\n",
    "    #     brVal = torch.where((brRow < 0) | (brCol < 0), 0, integral_image[..., brRow, brCol])\n",
    "    #     #print(brVal.shape,tlVal.shape,trVal.shape,blVal.shape)\n",
    "    #     # Compute the output\n",
    "    #     output = brVal + tlVal - trVal - blVal  # in-place addition\n",
    "    #     #output = output*self.box_filters[0].alpha\n",
    "    #     #print(output.shape,box_filters_alpha.shape)\n",
    "    #     #print(\"1:\",output)\n",
    "    #     output = output.permute(0, 1, 4, 2, 3)\n",
    "    #     #print(\"out:\",output.shape[2])\n",
    "    #     #box_filters_alpha = box_filters_alpha.view(1, 1, output.shape[2], 1, 1)\n",
    "    #     box_filters_alpha = box_filters_alpha.view(1, 1, -1, 1, 1)\n",
    "    #     output = output * box_filters_alpha\n",
    "    #     output = output.sum(dim=2)\n",
    "    #     #print(\"2:\",output)\n",
    "    #     return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxFilterConvolution(nn.Module):\n",
    "    def __init__(self, box_filters, ogFilterSize,reg,device):\n",
    "        super(BoxFilterConvolution, self).__init__()\n",
    "        self.box_filters = box_filters\n",
    "        self.ogFilterSize = ogFilterSize\n",
    "        self.reg = reg\n",
    "        self.device = device \n",
    " \n",
    "    def forward(self, integral_image):\n",
    "        \n",
    "        H, W = integral_image.shape[-2:]\n",
    "\n",
    "        H = H - self.ogFilterSize + 1\n",
    "        W = W - self.ogFilterSize + 1\n",
    "        #output = torch.zeros((integral_image.shape[0],integral_image.shape[1],H,W)).to(self.device)\n",
    "        # Define the boundaries\n",
    "        row_indices = torch.arange(H).unsqueeze(-1).unsqueeze(-1).to(self.device)  # shape: (H, 1, 1)\n",
    "        col_indices = torch.arange(W).unsqueeze(-1).unsqueeze(0).to(self.device)  # shape: (1, W, 1)\n",
    "       # print(row_indices.shape,col_indices.shape)\n",
    "        # a, b, c, d should be 1D tensors of shape (num_filters,)\n",
    "        # reshape them to (1, 1, num_filters)\n",
    "\n",
    "        #box_filters_tensor = torch.tensor([[int(box_filter.a), int(box_filter.b), int(box_filter.c), int(box_filter.d)] for box_filter in self.box_filters]).to(self.device)\n",
    "        box_filters_tensor = torch.tensor([[box_filter.a, box_filter.b, box_filter.c, box_filter.d] for box_filter in self.box_filters]).to(self.device)\n",
    "        \n",
    "        \n",
    "\n",
    "        a, b, c, d = box_filters_tensor[:, 0], box_filters_tensor[:, 1], box_filters_tensor[:, 2], box_filters_tensor[:, 3]\n",
    "        \n",
    "        a, b, c, d = a.unsqueeze(0).unsqueeze(0), b.unsqueeze(0).unsqueeze(0), c.unsqueeze(0).unsqueeze(0), d.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        #box_filters_alpha = torch.tensor([[box_filter.alpha] for box_filter in self.box_filters]).to(self.device)\n",
    "        box_filters_alpha = torch.stack([box_filter.alpha for box_filter in self.box_filters]).to(self.device)\n",
    "        #print(\"Alpha Check:\",box_filters_alpha,box_filters_alpha1)\n",
    "        #print(\"dtype\",box_filters_alpha.dtype)\n",
    "        print(\"CA Shape:\",col_indices.shape,a.shape)\n",
    "        tlCol = col_indices + a - 1  # broadcasting happens here\n",
    "        tlRow = row_indices + c - 1\n",
    "\n",
    "        trCol = col_indices + b\n",
    "        trRow = tlRow  # trRow and tlRow are the same\n",
    "\n",
    "        blCol = tlCol  # blCol and tlCol are the same\n",
    "        blRow = row_indices + d\n",
    "\n",
    "        brCol = trCol  # brCol and trCol are the same\n",
    "        brRow = blRow  # brRow and blRow are the same\n",
    "        #print(tlCol.shape,tlRow.shape,trCol.shape,trRow.shape,blCol.shape,blRow.shape,brCol.shape,brRow.shape)\n",
    "        # Get the values at the boundaries\n",
    "        tlVal = torch.where((tlRow < 0) | (tlCol < 0), 0, integral_image[..., tlRow, tlCol])\n",
    "        trVal = torch.where((trRow < 0) | (trCol < 0), 0, integral_image[..., trRow, trCol])\n",
    "        blVal = torch.where((blRow < 0) | (blCol < 0), 0, integral_image[..., blRow, blCol])\n",
    "        brVal = torch.where((brRow < 0) | (brCol < 0), 0, integral_image[..., brRow, brCol])\n",
    "        #print(brVal.shape,tlVal.shape,trVal.shape,blVal.shape)\n",
    "        # Compute the output\n",
    "        output = brVal + tlVal - trVal - blVal  # in-place addition\n",
    "        #output = output*self.box_filters[0].alpha\n",
    "        #print(output.shape,box_filters_alpha.shape)\n",
    "        #print(\"1:\",output)\n",
    "        output = output.permute(0, 1, 4, 2, 3)\n",
    "        #print(\"out:\",output.shape[2])\n",
    "        #box_filters_alpha = box_filters_alpha.view(1, 1, output.shape[2], 1, 1)\n",
    "        box_filters_alpha = box_filters_alpha.view(1, 1, -1, 1, 1)\n",
    "        output = output * box_filters_alpha\n",
    "        output = output.sum(dim=2)\n",
    "        #print(\"2:\",output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# CNN Network with box filters\n",
    "class cnnBox(nn.Module):\n",
    "\n",
    "    def __init__(self, nBoxes, nChannels, reg,device,box_filters,ogFilterSize):\n",
    "        super(cnnBox, self).__init__()\n",
    "        self.boxes = nBoxes\n",
    "        self.nChannels = nChannels\n",
    "        \n",
    "        #self.boxes = nn.ModuleList([boxFilters(0,1,0,1) for _ in range(self.boxes)])\n",
    "        #n = 5  # replace with the desired number of module lists\n",
    "        self.boxes = nn.ModuleList([boxFilters(0,1,0,1) for _ in range(self.boxes)])\n",
    "        for i in range(0,nBoxes):\n",
    "\n",
    "            self.boxes[i].setABCD(box_filters[i][0],box_filters[i][1],box_filters[i][2],box_filters[i][3])\n",
    "        \n",
    "        self.conv = BoxFilterConvolution(self.boxes,ogFilterSize,reg,device)\n",
    "        self.fc = nn.Linear(576,128)\n",
    "            \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.cumsum(dim=-2).cumsum(dim=-1)\n",
    "        x = self.conv(x)\n",
    "        x = x.to(torch.float)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m boxFiltersList \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m cBox \u001b[38;5;241m=\u001b[39m cnnBox(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[43mdevice\u001b[49m,boxFiltersList,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m cBox\u001b[38;5;241m.\u001b[39mboxes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msetAlpha(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(cBox\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "boxFiltersList = [[0,1,0,1],[0,1,0,1],[0,2,2,2]]\n",
    "cBox = cnnBox(3,1,False,device,boxFiltersList,3)\n",
    "cBox.boxes[1].setAlpha(2.0)\n",
    "\n",
    "optimizer = torch.optim.Adam(cBox.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "cBox = cBox.to(device)\n",
    "\n",
    "\n",
    "a = a.to(device)\n",
    "\n",
    "x=cBox(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convolution using box filters (1 Integral Image, n box filters)\n",
    "# Need to write backprop for the forward pass \n",
    "class BoxFilterConvolution(nn.Module):\n",
    "    def __init__(self, box_filters, ogFilterShape,reg,device):\n",
    "        super(BoxFilterConvolution, self).__init__()\n",
    "        self.box_filters = box_filters\n",
    "        self.ogFilterShape = ogFilterShape\n",
    "        self.reg = reg\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, integral_image):\n",
    "        \n",
    "        H, W = integral_image.shape[-2:]\n",
    "\n",
    "        H = H - self.ogFilterShape[0] + 1\n",
    "        W = W - self.ogFilterShape[1] + 1\n",
    "        C = self.ogFilterShape[2]\n",
    "        #print(C)\n",
    "        #output = torch.zeros((integral_image.shape[0],integral_image.shape[1],H,W)).to(self.device)\n",
    "        # Define the boundaries\n",
    "        #row_indices = torch.arange(H).unsqueeze(-1).unsqueeze(-1).to(self.device)  # shape: (H, 1, 1)\n",
    "        #col_indices = torch.arange(W).unsqueeze(-1).unsqueeze(0).to(self.device)  # shape: (1, W, 1)\n",
    "        row_indices = torch.arange(H).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).expand(C, H, 1, 1).to(self.device)  # shape: (C, H, 1, 1)\n",
    "        col_indices = torch.arange(W).unsqueeze(-1).unsqueeze(0).unsqueeze(0).expand(C, 1, W, 1).to(self.device)  # shape: (C, 1, W, 1)\n",
    "        #print(row_indices)\n",
    "       # print(row_indices.shape,col_indices.shape)\n",
    "        # a, b, c, d should be 1D tensors of shape (num_filters,)\n",
    "        # reshape them to (1, 1, num_filters)\n",
    "\n",
    "        #box_filters_tensor = torch.tensor([[int(box_filter.a), int(box_filter.b), int(box_filter.c), int(box_filter.d)] for box_filter in self.box_filters]).to(self.device)\n",
    "        #box_filters_tensor = torch.tensor([[box_filter.a, box_filter.b, box_filter.c, box_filter.d] for box_filter in self.box_filters]).to(self.device)\n",
    "        \n",
    "        box_filters_tensor = torch.tensor([[[box_filter.a, box_filter.b, box_filter.c, box_filter.d] for box_filter in sublist] for sublist in self.box_filters]).to(self.device)\n",
    "        #print(box_filters_tensor.shape,box_filters_tensor,box_filters_tensor[:,:,0])\n",
    "        a, b, c, d = box_filters_tensor[:,:, 0], box_filters_tensor[:,:, 1], box_filters_tensor[:,:, 2], box_filters_tensor[:,:, 3]\n",
    "        \n",
    "       \n",
    "        a, b, c, d = a.unsqueeze(1).unsqueeze(1), b.unsqueeze(1).unsqueeze(1), c.unsqueeze(1).unsqueeze(1), d.unsqueeze(1).unsqueeze(1)\n",
    "        #print(row_indices.shape,a.shape)\n",
    "        #return\n",
    "        #print(a,a.shape)\n",
    "        #return\n",
    "        #box_filters_alpha = torch.tensor([[box_filter.alpha] for box_filter in self.box_filters]).to(self.device)\n",
    "        #box_filters_alpha = torch.stack([box_filter.alpha for box_filter in self.box_filters]).to(self.device)\n",
    "        box_filters_alpha = torch.stack([torch.stack([box_filter.alpha for box_filter in sublist]) for sublist in self.box_filters]).to(self.device)\n",
    "        #print(\"Alpha Check:\",box_filters_alpha,box_filters_alpha.shape)\n",
    "        #print(col_indices.shape,a.shape)\n",
    "        #return\n",
    "        #print(\"dtype\",box_filters_alpha.dtype)\n",
    "        #print(\"CA Shape:\",col_indices.shape,a.shape)\n",
    "        tlCol = col_indices + a - 1  # broadcasting happens here\n",
    "        tlRow = row_indices + c - 1\n",
    "\n",
    "        trCol = col_indices + b\n",
    "        trRow = tlRow  # trRow and tlRow are the same\n",
    "\n",
    "        blCol = tlCol  # blCol and tlCol are the same\n",
    "        blRow = row_indices + d\n",
    "\n",
    "        brCol = trCol  # brCol and trCol are the same\n",
    "        brRow = blRow  # brRow and blRow are the same\n",
    "        #print(tlCol.shape,tlRow.shape,trCol.shape,trRow.shape,blCol.shape,blRow.shape,brCol.shape,brRow.shape)\n",
    "        # Get the values at the boundaries\n",
    "        tlVal = torch.where((tlRow < 0) | (tlCol < 0), 0, integral_image[..., tlRow, tlCol])\n",
    "        trVal = torch.where((trRow < 0) | (trCol < 0), 0, integral_image[..., trRow, trCol])\n",
    "        blVal = torch.where((blRow < 0) | (blCol < 0), 0, integral_image[..., blRow, blCol])\n",
    "        brVal = torch.where((brRow < 0) | (brCol < 0), 0, integral_image[..., brRow, brCol])\n",
    "        #print(brVal.shape,tlVal.shape,trVal.shape,blVal.shape)\n",
    "        # Compute the output\n",
    "        output = brVal + tlVal - trVal - blVal  # in-place addition\n",
    "        #output = output*self.box_filters[0].alpha\n",
    "        #print(output.shape,box_filters_alpha.shape)\n",
    "        #print(\"1:\",output)\n",
    "        \n",
    "        # while len(output.shape) > 5:\n",
    "        #     print(output.shape)\n",
    "        #     output = output.squeeze(0)\n",
    "        #print(\"output:\",output.shape)\n",
    "        output = output.squeeze(1)\n",
    "        output = output.permute(0, 1, 4, 2, 3)\n",
    "        #print(output)\n",
    "        #print(\"out:\",output.shape[2])\n",
    "        #box_filters_alpha = box_filters_alpha.view(1, 1, output.shape[2], 1, 1)\n",
    "        #print(box_filters_alpha.shape)\n",
    "        box_filters_alpha = box_filters_alpha.view(1, box_filters_alpha.shape[0], box_filters_alpha.shape[1], 1, 1)\n",
    "        output = output * box_filters_alpha\n",
    "        output = output.sum(dim=2)\n",
    "        #print(output)\n",
    "        #print(\"2:\",output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# CNN Network with box filters\n",
    "class cnnBox(nn.Module):\n",
    "\n",
    "    def __init__(self, nBoxes, reg,device,box_filters,ogFilterSize):\n",
    "        super(cnnBox, self).__init__()\n",
    "        self.boxes = nBoxes\n",
    "        self.nChannels = ogFilterSize[2]\n",
    "        \n",
    "        #self.boxes = nn.ModuleList([boxFilters(0,1,0,1) for _ in range(self.boxes)])\n",
    "        #n = 5  # replace with the desired number of module lists\n",
    "        self.boxes = [nn.ModuleList([boxFilters(0,1,0,1) for _ in range(self.boxes)]) for _ in range(self.nChannels)]\n",
    "        for i in range(0,self.nChannels):\n",
    "         for j in range(0,nBoxes):\n",
    "            self.boxes[i][j].setABCD(box_filters[i][j][0],box_filters[i][j][1],box_filters[i][j][2],box_filters[i][j][3])\n",
    "        \n",
    "        self.conv = BoxFilterConvolution(self.boxes,ogFilterSize,reg,device)\n",
    "        self.fc = nn.Linear(1728,128)\n",
    "            \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.cumsum(dim=-2).cumsum(dim=-1)\n",
    "        x = self.conv(x)\n",
    "        x = x.to(torch.float)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxFiltersList = [[[0,1,0,1],[0,1,0,1],[0,2,2,2]],[[0,2,0,2],[0,1,0,1],[0,2,2,2]]]\n",
    "cBox = cnnBox(3,False,device,boxFiltersList,(3,3,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(1,5,(2,1,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4, 1, 2, 1, 2, 3, 1, 2, 2, 3],\n",
       "          [1, 3, 3, 3, 3, 4, 3, 2, 2, 2],\n",
       "          [4, 3, 4, 1, 4, 4, 1, 4, 1, 2],\n",
       "          [3, 1, 4, 2, 3, 2, 3, 4, 3, 3],\n",
       "          [4, 1, 4, 4, 2, 2, 2, 1, 1, 4],\n",
       "          [2, 3, 1, 4, 4, 2, 3, 1, 1, 3],\n",
       "          [3, 3, 2, 2, 3, 2, 4, 2, 3, 4],\n",
       "          [2, 3, 2, 3, 2, 4, 3, 2, 2, 4],\n",
       "          [4, 1, 4, 1, 3, 2, 2, 2, 3, 1],\n",
       "          [4, 2, 2, 2, 1, 3, 4, 2, 1, 3]]],\n",
       "\n",
       "\n",
       "        [[[2, 3, 3, 2, 2, 3, 1, 2, 3, 2],\n",
       "          [2, 3, 3, 2, 2, 3, 3, 3, 1, 1],\n",
       "          [1, 1, 1, 2, 1, 3, 1, 1, 1, 4],\n",
       "          [3, 3, 3, 3, 1, 2, 1, 1, 3, 3],\n",
       "          [4, 4, 1, 2, 3, 3, 2, 1, 1, 3],\n",
       "          [3, 3, 1, 3, 3, 3, 3, 3, 1, 4],\n",
       "          [4, 3, 4, 2, 4, 2, 1, 3, 2, 4],\n",
       "          [1, 3, 3, 1, 4, 4, 4, 3, 4, 1],\n",
       "          [4, 3, 2, 4, 3, 1, 2, 1, 2, 2],\n",
       "          [1, 3, 4, 3, 3, 1, 3, 4, 4, 2]]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[[[0]],\n",
      "\n",
      "         [[1]],\n",
      "\n",
      "         [[2]],\n",
      "\n",
      "         [[3]],\n",
      "\n",
      "         [[4]],\n",
      "\n",
      "         [[5]],\n",
      "\n",
      "         [[6]],\n",
      "\n",
      "         [[7]]],\n",
      "\n",
      "\n",
      "        [[[0]],\n",
      "\n",
      "         [[1]],\n",
      "\n",
      "         [[2]],\n",
      "\n",
      "         [[3]],\n",
      "\n",
      "         [[4]],\n",
      "\n",
      "         [[5]],\n",
      "\n",
      "         [[6]],\n",
      "\n",
      "         [[7]]]])\n",
      "torch.Size([2, 3, 4]) tensor([[[0, 1, 0, 1],\n",
      "         [0, 1, 0, 1],\n",
      "         [0, 2, 2, 2]],\n",
      "\n",
      "        [[0, 2, 0, 2],\n",
      "         [0, 1, 0, 1],\n",
      "         [0, 2, 2, 2]]]) tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.Size([2, 8, 1, 1]) torch.Size([2, 1, 1, 3])\n",
      "tensor([[[[0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0]]]]) torch.Size([2, 1, 1, 3])\n",
      "Alpha Check: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<StackBackward0>) torch.Size([2, 3])\n",
      "torch.Size([2, 1, 8, 1]) torch.Size([2, 1, 1, 3])\n",
      "torch.Size([2, 1, 8, 3]) torch.Size([2, 8, 1, 3]) torch.Size([2, 1, 8, 3]) torch.Size([2, 8, 1, 3]) torch.Size([2, 1, 8, 3]) torch.Size([2, 8, 1, 3]) torch.Size([2, 1, 8, 3]) torch.Size([2, 8, 1, 3])\n",
      "torch.Size([2, 1, 2, 8, 8, 3]) torch.Size([2, 1, 2, 8, 8, 3]) torch.Size([2, 1, 2, 8, 8, 3]) torch.Size([2, 1, 2, 8, 8, 3])\n",
      "output: torch.Size([2, 1, 2, 8, 8, 3])\n",
      "tensor([[[[[ 9,  9,  9,  9, 12, 11,  8,  8],\n",
      "           [11, 13, 11, 11, 15, 12, 10,  9],\n",
      "           [11, 12, 11, 10, 13, 10, 12, 12],\n",
      "           [ 9, 10, 14, 11,  9,  9, 10,  9],\n",
      "           [10,  9, 13, 14, 10,  9,  7,  4],\n",
      "           [11,  9,  9, 13, 11, 11, 10,  7],\n",
      "           [11, 10,  9, 10, 11, 13, 11,  9],\n",
      "           [10, 10, 10,  9, 11, 11,  9,  9]],\n",
      "\n",
      "          [[ 9,  9,  9,  9, 12, 11,  8,  8],\n",
      "           [11, 13, 11, 11, 15, 12, 10,  9],\n",
      "           [11, 12, 11, 10, 13, 10, 12, 12],\n",
      "           [ 9, 10, 14, 11,  9,  9, 10,  9],\n",
      "           [10,  9, 13, 14, 10,  9,  7,  4],\n",
      "           [11,  9,  9, 13, 11, 11, 10,  7],\n",
      "           [11, 10,  9, 10, 11, 13, 11,  9],\n",
      "           [10, 10, 10,  9, 11, 11,  9,  9]],\n",
      "\n",
      "          [[11,  8,  9,  9,  9,  9,  6,  7],\n",
      "           [ 8,  7,  9,  7,  8,  9, 10, 10],\n",
      "           [ 9,  9, 10,  8,  6,  5,  4,  6],\n",
      "           [ 6,  8,  9, 10,  9,  6,  5,  5],\n",
      "           [ 8,  7,  7,  7,  9,  8,  9,  9],\n",
      "           [ 7,  8,  7,  9,  9,  9,  7,  8],\n",
      "           [ 9,  6,  8,  6,  7,  6,  7,  6],\n",
      "           [ 8,  6,  5,  6,  8,  9,  7,  6]]],\n",
      "\n",
      "\n",
      "         [[[25, 21, 23, 25, 25, 24, 18, 20],\n",
      "           [26, 24, 27, 26, 27, 27, 23, 23],\n",
      "           [28, 24, 28, 24, 23, 23, 20, 23],\n",
      "           [23, 24, 28, 25, 23, 20, 19, 21],\n",
      "           [23, 24, 26, 25, 24, 19, 18, 20],\n",
      "           [21, 23, 23, 26, 27, 23, 21, 22],\n",
      "           [24, 21, 22, 22, 25, 23, 23, 23],\n",
      "           [24, 20, 20, 21, 24, 24, 21, 20]],\n",
      "\n",
      "          [[ 9,  9,  9,  9, 12, 11,  8,  8],\n",
      "           [11, 13, 11, 11, 15, 12, 10,  9],\n",
      "           [11, 12, 11, 10, 13, 10, 12, 12],\n",
      "           [ 9, 10, 14, 11,  9,  9, 10,  9],\n",
      "           [10,  9, 13, 14, 10,  9,  7,  4],\n",
      "           [11,  9,  9, 13, 11, 11, 10,  7],\n",
      "           [11, 10,  9, 10, 11, 13, 11,  9],\n",
      "           [10, 10, 10,  9, 11, 11,  9,  9]],\n",
      "\n",
      "          [[11,  8,  9,  9,  9,  9,  6,  7],\n",
      "           [ 8,  7,  9,  7,  8,  9, 10, 10],\n",
      "           [ 9,  9, 10,  8,  6,  5,  4,  6],\n",
      "           [ 6,  8,  9, 10,  9,  6,  5,  5],\n",
      "           [ 8,  7,  7,  7,  9,  8,  9,  9],\n",
      "           [ 7,  8,  7,  9,  9,  9,  7,  8],\n",
      "           [ 9,  6,  8,  6,  7,  6,  7,  6],\n",
      "           [ 8,  6,  5,  6,  8,  9,  7,  6]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[10, 12, 10,  8, 10, 10,  9,  9],\n",
      "           [ 7,  8,  8,  7,  9, 10,  8,  6],\n",
      "           [ 8,  8,  9,  7,  7,  7,  4,  6],\n",
      "           [14, 11,  9,  9,  9,  8,  5,  6],\n",
      "           [14,  9,  7, 11, 12, 11,  9,  6],\n",
      "           [13, 11, 10, 12, 12,  9, 10,  9],\n",
      "           [11, 13, 10, 11, 14, 11, 11, 12],\n",
      "           [11, 11, 10, 12, 12, 11, 10, 10]],\n",
      "\n",
      "          [[10, 12, 10,  8, 10, 10,  9,  9],\n",
      "           [ 7,  8,  8,  7,  9, 10,  8,  6],\n",
      "           [ 8,  8,  9,  7,  7,  7,  4,  6],\n",
      "           [14, 11,  9,  9,  9,  8,  5,  6],\n",
      "           [14,  9,  7, 11, 12, 11,  9,  6],\n",
      "           [13, 11, 10, 12, 12,  9, 10,  9],\n",
      "           [11, 13, 10, 11, 14, 11, 11, 12],\n",
      "           [11, 11, 10, 12, 12, 11, 10, 10]],\n",
      "\n",
      "          [[ 3,  4,  4,  6,  5,  5,  3,  6],\n",
      "           [ 9,  9,  7,  6,  4,  4,  5,  7],\n",
      "           [ 9,  7,  6,  8,  8,  6,  4,  5],\n",
      "           [ 7,  7,  7,  9,  9,  9,  7,  8],\n",
      "           [11,  9, 10,  8,  7,  6,  6,  9],\n",
      "           [ 7,  7,  8,  9, 12, 11, 11,  8],\n",
      "           [ 9,  9,  9,  8,  6,  4,  5,  5],\n",
      "           [ 8, 10, 10,  7,  7,  8, 11, 10]]],\n",
      "\n",
      "\n",
      "         [[[19, 20, 18, 20, 19, 20, 16, 18],\n",
      "           [20, 21, 18, 19, 17, 18, 15, 18],\n",
      "           [21, 20, 17, 20, 17, 15, 12, 18],\n",
      "           [25, 23, 20, 23, 21, 19, 16, 20],\n",
      "           [27, 23, 23, 25, 24, 21, 17, 22],\n",
      "           [25, 23, 25, 26, 28, 26, 24, 25],\n",
      "           [27, 25, 27, 25, 25, 21, 22, 22],\n",
      "           [24, 26, 27, 24, 25, 23, 27, 23]],\n",
      "\n",
      "          [[10, 12, 10,  8, 10, 10,  9,  9],\n",
      "           [ 7,  8,  8,  7,  9, 10,  8,  6],\n",
      "           [ 8,  8,  9,  7,  7,  7,  4,  6],\n",
      "           [14, 11,  9,  9,  9,  8,  5,  6],\n",
      "           [14,  9,  7, 11, 12, 11,  9,  6],\n",
      "           [13, 11, 10, 12, 12,  9, 10,  9],\n",
      "           [11, 13, 10, 11, 14, 11, 11, 12],\n",
      "           [11, 11, 10, 12, 12, 11, 10, 10]],\n",
      "\n",
      "          [[ 3,  4,  4,  6,  5,  5,  3,  6],\n",
      "           [ 9,  9,  7,  6,  4,  4,  5,  7],\n",
      "           [ 9,  7,  6,  8,  8,  6,  4,  5],\n",
      "           [ 7,  7,  7,  9,  9,  9,  7,  8],\n",
      "           [11,  9, 10,  8,  7,  6,  6,  9],\n",
      "           [ 7,  7,  8,  9, 12, 11, 11,  8],\n",
      "           [ 9,  9,  9,  8,  6,  4,  5,  5],\n",
      "           [ 8, 10, 10,  7,  7,  8, 11, 10]]]]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[[29., 26., 27., 27., 33., 31., 22., 23.],\n",
      "          [30., 33., 31., 29., 38., 33., 30., 28.],\n",
      "          [31., 33., 32., 28., 32., 25., 28., 30.],\n",
      "          [24., 28., 37., 32., 27., 24., 25., 23.],\n",
      "          [28., 25., 33., 35., 29., 26., 23., 17.],\n",
      "          [29., 26., 25., 35., 31., 31., 27., 22.],\n",
      "          [31., 26., 26., 26., 29., 32., 29., 24.],\n",
      "          [28., 26., 25., 24., 30., 31., 25., 24.]],\n",
      "\n",
      "         [[45., 38., 41., 43., 46., 44., 32., 35.],\n",
      "          [45., 44., 47., 44., 50., 48., 43., 42.],\n",
      "          [48., 45., 49., 42., 42., 38., 36., 41.],\n",
      "          [38., 42., 51., 46., 41., 35., 34., 35.],\n",
      "          [41., 40., 46., 46., 43., 36., 34., 33.],\n",
      "          [39., 40., 39., 48., 47., 43., 38., 37.],\n",
      "          [44., 37., 39., 38., 43., 42., 41., 38.],\n",
      "          [42., 36., 35., 36., 43., 44., 37., 35.]]],\n",
      "\n",
      "\n",
      "        [[[23., 28., 24., 22., 25., 25., 21., 24.],\n",
      "          [23., 25., 23., 20., 22., 24., 21., 19.],\n",
      "          [25., 23., 24., 22., 22., 20., 12., 17.],\n",
      "          [35., 29., 25., 27., 27., 25., 17., 20.],\n",
      "          [39., 27., 24., 30., 31., 28., 24., 21.],\n",
      "          [33., 29., 28., 33., 36., 29., 31., 26.],\n",
      "          [31., 35., 29., 30., 34., 26., 27., 29.],\n",
      "          [30., 32., 30., 31., 31., 30., 31., 30.]],\n",
      "\n",
      "         [[32., 36., 32., 34., 34., 35., 28., 33.],\n",
      "          [36., 38., 33., 32., 30., 32., 28., 31.],\n",
      "          [38., 35., 32., 35., 32., 28., 20., 29.],\n",
      "          [46., 41., 36., 41., 39., 36., 28., 34.],\n",
      "          [52., 41., 40., 44., 43., 38., 32., 37.],\n",
      "          [45., 41., 43., 47., 52., 46., 45., 42.],\n",
      "          [47., 47., 46., 44., 45., 36., 38., 39.],\n",
      "          [43., 47., 47., 43., 44., 42., 48., 43.]]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cBox = cBox.to(device)\n",
    "\n",
    "\n",
    "a = a.to(device)\n",
    "\n",
    "x=cBox(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2456e+01,  4.2573e+00, -1.7557e+01,  2.8268e+00,  3.9563e+01,\n",
      "         -1.1630e+01,  2.0830e+00, -2.1055e+00,  2.2533e+01,  2.1942e+01,\n",
      "          1.6660e+01, -1.7088e+01,  1.1688e+01, -7.5054e-01, -1.0601e+01,\n",
      "          6.1962e+00, -5.0074e+00,  1.3174e+01, -2.9013e+01, -7.4227e+00,\n",
      "         -2.5886e+01,  7.4658e+00, -1.2978e+00, -1.9370e+01, -3.4568e+01,\n",
      "         -2.3533e+01,  2.4840e+01,  1.1263e+01, -3.1078e+01,  2.8998e+01,\n",
      "         -1.2457e+01, -1.7536e+00,  1.5896e+01,  4.4723e+01,  1.0898e+01,\n",
      "         -3.3288e+01,  8.8117e+00, -7.7372e+00, -2.0122e+01, -2.4207e+00,\n",
      "          9.6303e+00, -1.2655e+01,  9.0999e+00,  7.1533e+00, -3.4593e+01,\n",
      "         -7.8110e+00,  3.6909e+01, -2.4225e+00, -1.5447e+01, -3.5261e+00,\n",
      "         -2.5960e+01,  3.1802e-02,  1.1006e+01,  2.3645e+01,  1.3435e+00,\n",
      "         -2.3995e+01, -3.2661e+01,  1.0590e+01,  4.1259e+00,  2.5446e+01,\n",
      "         -2.1179e+01, -1.3774e+01, -1.3439e+01, -1.2278e+01,  1.2628e+01,\n",
      "          1.4631e+01,  3.5036e+01,  2.5935e+01,  1.2908e+01,  3.8960e+01,\n",
      "         -4.0143e+00, -3.2252e+01,  8.7843e+00,  4.6811e+01,  7.2889e+00,\n",
      "          6.0900e-02, -1.3673e+01, -2.4631e+01,  1.4390e+01,  2.9413e+01,\n",
      "          4.0450e+01, -9.7784e+00,  6.6920e+00,  9.9691e+00, -1.0101e+01,\n",
      "          1.6371e+00, -1.2145e+01,  2.6901e+01,  2.4197e+01,  4.6777e+00,\n",
      "         -1.5080e+01,  2.3322e+01, -1.4316e+01, -1.8506e+01, -5.3633e+00,\n",
      "          6.5531e+00,  2.0153e+01, -1.6515e+01, -2.8623e+01,  1.2490e+01,\n",
      "          2.7551e+01, -9.0066e+00,  2.7360e+00,  4.8978e+01, -2.3973e+01,\n",
      "          2.4655e+01,  1.8800e+01,  1.9464e+01,  1.8210e+01,  1.1162e+01,\n",
      "         -6.4907e+00,  1.0951e+01, -3.8878e+00,  9.0215e+00,  5.5943e+01,\n",
      "         -3.0340e+01,  2.5685e+01,  1.6816e+01, -3.7649e+00, -5.9786e+00,\n",
      "          1.5910e+01,  1.6644e+01,  5.7405e+00, -2.2750e+01,  1.7822e+01,\n",
      "          3.1806e+01,  1.2413e+01, -2.8113e+01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:0\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((1,1,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTempFilter(tempBoxFilterList):\n",
    "    tempFilter = np.zeros((3,3))\n",
    "    for i in range(0,len(tempBoxFilterList)):\n",
    "        for j in range(tempBoxFilterList[i].c,tempBoxFilterList[i].d+1):\n",
    "            for k in range(tempBoxFilterList[i].a,tempBoxFilterList[i].b+1):\n",
    "                #print(j,k)\n",
    "                tempFilter[j][k] = tempFilter[j][k] + tempBoxFilterList[i].alpha\n",
    "\n",
    "    return tempFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x boxFilters()\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cBox.boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFilter1 = computeTempFilter(cBox.boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 0.],\n",
       "       [2., 2., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempFilter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFilter2 = computeTempFilter(cBox.boxes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 10])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 0.]\n",
      " [2. 2. 0.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[23., 28., 24., 22., 25., 25., 21., 24.],\n",
      "        [23., 25., 23., 20., 22., 24., 21., 19.],\n",
      "        [25., 23., 24., 22., 22., 20., 12., 17.],\n",
      "        [35., 29., 25., 27., 27., 25., 17., 20.],\n",
      "        [39., 27., 24., 30., 31., 28., 24., 21.],\n",
      "        [33., 29., 28., 33., 36., 29., 31., 26.],\n",
      "        [31., 35., 29., 30., 34., 26., 27., 29.],\n",
      "        [30., 32., 30., 31., 31., 30., 31., 30.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "# Define a 2D kernel for the convolution\n",
    "kernel = np.array(tempFilter1)\n",
    "print(kernel)\n",
    "# Perform the 2D convolution\n",
    "output_array = torch.tensor(convolve2d(a[1][0], np.flip(kernel), mode='valid'))\n",
    "\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 8, 8])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 10])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29., 26., 27., 27., 33., 31., 22., 23.],\n",
       "        [30., 33., 31., 29., 38., 33., 30., 28.],\n",
       "        [31., 33., 32., 28., 32., 25., 28., 30.],\n",
       "        [24., 28., 37., 32., 27., 24., 25., 23.],\n",
       "        [28., 25., 33., 35., 29., 26., 23., 17.],\n",
       "        [29., 26., 25., 35., 31., 31., 27., 22.],\n",
       "        [31., 26., 26., 26., 29., 32., 29., 24.],\n",
       "        [28., 26., 25., 24., 30., 31., 25., 24.]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "error = x[1][0] - output_array\n",
    "\n",
    "print(torch.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed 1281448.208694458\n",
      "Epoch 2 completed 608667.748123169\n",
      "Epoch 3 completed 543034.5018463135\n",
      "Epoch 4 completed 545290.6164245605\n",
      "Epoch 5 completed 539991.2645645142\n",
      "Epoch 6 completed 525097.845664978\n",
      "Epoch 7 completed 492830.871673584\n",
      "Epoch 8 completed 511459.7568283081\n",
      "Epoch 9 completed 526388.3687419891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, labels)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[180], line 118\u001b[0m, in \u001b[0;36mcnnBox.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[180], line 69\u001b[0m, in \u001b[0;36mBoxFilterConvolution.forward\u001b[0;34m(self, integral_image)\u001b[0m\n\u001b[1;32m     67\u001b[0m trVal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere((trRow \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (trCol \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m0\u001b[39m, integral_image[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, trRow, trCol])\n\u001b[1;32m     68\u001b[0m blVal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere((blRow \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (blCol \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m0\u001b[39m, integral_image[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, blRow, blCol])\n\u001b[0;32m---> 69\u001b[0m brVal \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrRow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegral_image\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrRow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrCol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#print(brVal.shape,tlVal.shape,trVal.shape,blVal.shape)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Compute the output\u001b[39;00m\n\u001b[1;32m     72\u001b[0m output \u001b[38;5;241m=\u001b[39m brVal \u001b[38;5;241m+\u001b[39m tlVal \u001b[38;5;241m-\u001b[39m trVal \u001b[38;5;241m-\u001b[39m blVal  \u001b[38;5;66;03m# in-place addition\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "boxFiltersList = [[[0,1,0,1],[0,2,0,2],[0,3,0,3],[0,4,0,4],[1,3,1,3],[1,4,1,4],[2,4,2,4],[3,4,3,4],[0,4,0,0],[0,0,0,4],[2,2,0,4],[0,4,2,2]],\n",
    "                 [[0,1,0,2],[0,2,0,3],[0,3,0,4],[0,4,0,1],[1,3,1,4],[1,4,1,0],[2,4,2,1],[3,4,3,2],[0,4,0,3],[0,0,0,2],[2,2,0,3],[0,4,2,1]],\n",
    "                  [[0,1,0,3],[0,2,0,4],[0,3,0,1],[0,4,0,2],[1,3,1,0],[1,4,1,2],[2,4,2,3],[3,4,3,0],[0,4,0,1],[0,0,0,3],[2,2,0,1],[0,4,2,3]]]\n",
    "\n",
    "cBox = cnnBox(12,False,device,boxFiltersList,(5,5,3))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cBox.to(device)\n",
    "optimizer = optim.SGD(cBox.parameters(), lr=0.01)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(20):  # 10 epochs\n",
    "    totalLoss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = cBox(images)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalLoss = totalLoss + loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} completed\",totalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTempFilter(tempBoxFilterList):\n",
    "    tempFilter = np.zeros((5,5))\n",
    "    for i in range(0,len(tempBoxFilterList)):\n",
    "        for j in range(tempBoxFilterList[i].c,tempBoxFilterList[i].d+1):\n",
    "            for k in range(tempBoxFilterList[i].a,tempBoxFilterList[i].b+1):\n",
    "                #print(j,k)\n",
    "                tempFilter[j][k] = tempFilter[j][k] + tempBoxFilterList[i].alpha\n",
    "\n",
    "    return tempFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.13670456  0.58637291  0.64402074 -0.37863606 -0.32693762]\n",
      " [ 0.62322569  0.53287411  0.59052193 -0.43213487 -0.91188622]\n",
      " [ 0.75025058  0.659899    1.62777996  0.60512316  0.12537181]\n",
      " [-0.34178329 -0.43213487  0.5357461   0.50745147  0.02770013]\n",
      " [-0.29008484 -0.91188622  0.05599475  0.02770013  0.02770013]]\n"
     ]
    }
   ],
   "source": [
    "tempFilter = computeTempFilter(cBox.boxes)\n",
    "\n",
    "print(tempFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0 B: 1 C: 0 D: 1 Alpha: Parameter containing:\n",
      "tensor(0.6102, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 2 C: 0 D: 2 Alpha: Parameter containing:\n",
      "tensor(0.3548, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 3 C: 0 D: 3 Alpha: Parameter containing:\n",
      "tensor(-0.0517, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 4 C: 0 D: 4 Alpha: Parameter containing:\n",
      "tensor(-0.8404, device='cuda:0', requires_grad=True)\n",
      "A: 1 B: 3 C: 1 D: 3 Alpha: Parameter containing:\n",
      "tensor(0.5314, device='cuda:0', requires_grad=True)\n",
      "A: 1 B: 4 C: 1 D: 4 Alpha: Parameter containing:\n",
      "tensor(-0.0715, device='cuda:0', requires_grad=True)\n",
      "A: 2 B: 4 C: 2 D: 4 Alpha: Parameter containing:\n",
      "tensor(0.3000, device='cuda:0', requires_grad=True)\n",
      "A: 3 B: 4 C: 3 D: 4 Alpha: Parameter containing:\n",
      "tensor(0.6396, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 4 C: 0 D: 0 Alpha: Parameter containing:\n",
      "tensor(0.5135, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 0 C: 0 D: 4 Alpha: Parameter containing:\n",
      "tensor(0.5503, device='cuda:0', requires_grad=True)\n",
      "A: 2 B: 2 C: 0 D: 4 Alpha: Parameter containing:\n",
      "tensor(0.6679, device='cuda:0', requires_grad=True)\n",
      "A: 0 B: 4 C: 2 D: 2 Alpha: Parameter containing:\n",
      "tensor(0.7372, device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in cBox.boxes:\n",
    "    i.printParams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 89 %\n"
     ]
    }
   ],
   "source": [
    "cBox.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cBox(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Epoch 4 completed\n",
      "Epoch 5 completed\n",
      "Epoch 6 completed\n",
      "Epoch 7 completed\n",
      "Epoch 8 completed\n",
      "Epoch 9 completed\n",
      "Epoch 10 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.fc = nn.Linear(1728, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        #x = x.view(-1, 36864)  # Flatten the tensor\n",
    "        #print(x.shape)\n",
    "        x = x.flatten(1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Net()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):  # 10 epochs\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = nn.CrossEntropyLoss()(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.5251,  0.1842,  0.2110,  0.1839,  0.3199],\n",
       "          [ 0.1764,  0.0774, -0.2433, -0.1485, -0.2381],\n",
       "          [-0.0945, -0.5673, -0.1638, -0.3058, -0.4650],\n",
       "          [-0.1649, -0.3075, -0.4817, -0.4778, -0.4577],\n",
       "          [ 0.2636, -0.1681, -0.1174, -0.1546, -0.0658]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(model.conv1.weight[0][0].detach().cpu()), cmap='gray')\n",
    "plt.colorbar()  # Optional: display a color bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(tempFilter), cmap='gray')\n",
    "plt.colorbar()  # Optional: display a color bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0,10,(1,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:0\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254, 1, 1]) torch.Size([1, 254, 1])\n",
      "torch.Size([1, 1, 100]) torch.Size([1, 1, 100]) torch.Size([1, 1, 100]) torch.Size([1, 1, 100])\n",
      "torch.Size([1, 254, 100]) torch.Size([254, 1, 100]) torch.Size([1, 254, 100]) torch.Size([254, 1, 100]) torch.Size([1, 254, 100]) torch.Size([254, 1, 100]) torch.Size([1, 254, 100]) torch.Size([254, 1, 100])\n",
      "torch.Size([1, 1, 254, 254, 100]) torch.Size([1, 1, 254, 254, 100]) torch.Size([1, 1, 254, 254, 100]) torch.Size([1, 1, 254, 254, 100])\n",
      "Time passed: 0.08255045022815466 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Record the start time\n",
    "\n",
    "\n",
    "cBox = cnnBox(100,True,device)\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    a = a.to(device)\n",
    "    x=cBox(a)\n",
    "\n",
    "#print(x)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Calculate the time passed\n",
    "time_passed = end_time - start_time\n",
    "\n",
    "print(f'Time passed: {time_passed/1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # Record the start time\n",
    "# cBox = cnnBox(1,False,device)\n",
    "# start_time = time.time()\n",
    "\n",
    "# a = a.to(device)\n",
    "# for i in range(1):\n",
    "#     x=cBox(a)\n",
    "\n",
    "# #print(x)\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate the time passed\n",
    "# time_passed = end_time - start_time\n",
    "\n",
    "# print(f'Time passed: {time_passed/1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 0.006571175064891577 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "conv2 = nn.Conv2d(1, 1, 5, bias=False).to(device)\n",
    "\n",
    "a = torch.rand((1,1,256,256)).to(device)\n",
    "a =a.to(device)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for i in range(1):\n",
    " conv2(a)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f'Time passed: {(end_time - start_time)/1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "convLayer = cnnBox(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convLayer.boxes[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes.0.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.0.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.0.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.0.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.0.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.1.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.1.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.1.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.1.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.1.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.2.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.2.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.2.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.2.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.2.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.3.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.3.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.3.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.3.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.3.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.4.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.4.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.4.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.4.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.4.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.5.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.5.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.5.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.5.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.5.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.6.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.6.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.6.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.6.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.6.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.7.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.7.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.7.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.7.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.7.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.8.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.8.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.8.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.8.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.8.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "boxes.9.alpha Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "boxes.9.a Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.9.b Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "boxes.9.c Parameter containing:\n",
      "tensor(2., requires_grad=True)\n",
      "boxes.9.d Parameter containing:\n",
      "tensor(6., requires_grad=True)\n",
      "fc.weight Parameter containing:\n",
      "tensor([[ 0.0258,  0.0074, -0.0099,  ..., -0.0305, -0.0256,  0.0086],\n",
      "        [ 0.0285, -0.0267, -0.0249,  ...,  0.0242, -0.0104,  0.0059],\n",
      "        [ 0.0158,  0.0266, -0.0220,  ...,  0.0109, -0.0004,  0.0115],\n",
      "        ...,\n",
      "        [ 0.0136, -0.0018, -0.0224,  ...,  0.0020, -0.0022,  0.0168],\n",
      "        [-0.0305, -0.0164,  0.0201,  ..., -0.0148,  0.0049,  0.0278],\n",
      "        [ 0.0099,  0.0269, -0.0085,  ..., -0.0121,  0.0185,  0.0008]],\n",
      "       requires_grad=True)\n",
      "fc.bias Parameter containing:\n",
      "tensor([-1.6741e-02, -9.9615e-03, -7.5686e-03, -9.3352e-03, -1.4505e-02,\n",
      "        -2.0810e-02,  2.0309e-02,  9.4824e-03,  3.2583e-04, -5.3416e-03,\n",
      "        -7.2174e-03, -7.9849e-03,  2.5219e-02,  3.0979e-03, -1.3545e-02,\n",
      "        -2.2223e-02, -7.6578e-03,  1.8603e-02, -2.1268e-02, -6.3241e-03,\n",
      "        -5.7276e-03, -5.2339e-03,  8.6179e-03, -5.7019e-03, -1.4784e-02,\n",
      "        -3.9743e-03, -1.3854e-02, -2.1746e-02, -1.0599e-02,  1.1584e-02,\n",
      "        -1.5871e-02, -5.3490e-03,  9.0959e-03,  7.1845e-03,  1.6387e-02,\n",
      "        -5.7288e-03,  2.4257e-02,  8.6002e-03,  2.0170e-03,  1.8489e-02,\n",
      "         6.9169e-03,  1.8462e-02, -2.0028e-02,  1.1529e-02, -1.4702e-02,\n",
      "        -2.0959e-02, -2.4157e-02,  1.3569e-02, -1.2005e-02,  1.9729e-02,\n",
      "         1.4127e-02, -9.9417e-03,  9.1080e-03,  2.5278e-02, -6.3888e-03,\n",
      "        -1.5245e-02, -1.9457e-05, -2.2031e-02, -1.9173e-02,  8.0463e-03,\n",
      "         2.5077e-02,  1.9417e-02,  1.4314e-02, -2.0607e-02,  3.8993e-03,\n",
      "        -2.4344e-02, -6.3146e-03,  4.7464e-03,  2.5948e-03,  1.8483e-02,\n",
      "        -7.2757e-03, -2.7203e-03, -2.2625e-02, -3.4848e-03,  1.4994e-02,\n",
      "        -1.1582e-02, -3.5645e-03,  4.5775e-04,  1.2888e-02, -6.3341e-03,\n",
      "         1.4635e-02,  8.5967e-03,  2.1021e-02,  1.6792e-02,  1.4807e-02,\n",
      "         1.9036e-02,  2.0242e-02,  1.7410e-02, -4.8041e-04, -2.2002e-02,\n",
      "        -2.1438e-02,  1.9672e-02, -3.9807e-03,  9.2758e-03, -6.4867e-03,\n",
      "        -1.8982e-03,  8.5789e-03, -9.7537e-03,  7.8397e-03, -6.0590e-03,\n",
      "         1.9790e-02, -8.8796e-03,  1.3696e-03, -1.8472e-02,  1.3124e-02,\n",
      "        -1.8756e-02,  1.8632e-02,  2.4429e-02,  9.0893e-03, -4.3882e-04,\n",
      "        -1.1061e-02,  2.1321e-03, -8.6589e-03, -1.1697e-02,  7.4866e-03,\n",
      "         7.5245e-03,  1.2141e-02, -1.6457e-02, -5.5572e-03, -6.1597e-03,\n",
      "        -1.5589e-02,  5.7600e-04,  7.5218e-03,  1.1424e-02, -1.0440e-02,\n",
      "         8.3684e-03,  2.1571e-02, -2.3421e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i,j in convLayer.named_parameters():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7407e-03,  3.8544e-05, -1.7569e-02,  6.6483e-04, -4.5048e-03,\n",
      "         -1.0810e-02,  1.0309e-02, -5.1756e-04, -9.6741e-03,  4.6584e-03,\n",
      "         -1.7217e-02, -1.7985e-02,  1.5219e-02,  1.3098e-02, -3.5450e-03,\n",
      "         -1.2223e-02,  2.3422e-03,  8.6034e-03, -1.1268e-02,  3.6759e-03,\n",
      "         -1.5728e-02, -1.5234e-02,  1.8618e-02,  4.2981e-03, -4.7841e-03,\n",
      "          6.0257e-03, -3.8544e-03, -1.1747e-02, -5.9944e-04,  1.5845e-03,\n",
      "         -5.8710e-03, -1.5349e-02, -9.0407e-04,  1.7184e-02,  6.3869e-03,\n",
      "         -1.5729e-02,  1.4257e-02, -1.3998e-03,  1.2017e-02,  8.4893e-03,\n",
      "          1.6917e-02,  8.4618e-03, -1.0028e-02,  1.5290e-03, -4.7018e-03,\n",
      "         -1.0959e-02, -1.4157e-02,  3.5690e-03, -2.0050e-03,  9.7287e-03,\n",
      "          4.1273e-03,  5.8265e-05, -8.9195e-04,  1.5278e-02, -1.6389e-02,\n",
      "         -5.2448e-03, -1.0019e-02, -1.2031e-02, -9.1731e-03,  1.8046e-02,\n",
      "          1.5077e-02,  9.4165e-03,  4.3142e-03, -1.0608e-02,  1.3899e-02,\n",
      "         -1.4344e-02, -1.6315e-02, -5.2535e-03,  1.2595e-02,  8.4834e-03,\n",
      "          2.7243e-03,  7.2797e-03, -1.2625e-02,  6.5152e-03,  4.9942e-03,\n",
      "         -1.5818e-03,  6.4355e-03, -9.5421e-03,  2.8881e-03, -1.6334e-02,\n",
      "          4.6346e-03, -1.4033e-03,  1.1021e-02,  6.7918e-03,  4.8070e-03,\n",
      "          9.0358e-03,  1.0242e-02,  7.4100e-03, -1.0480e-02, -1.2002e-02,\n",
      "         -1.1438e-02,  9.6718e-03,  6.0193e-03, -7.2414e-04,  3.5133e-03,\n",
      "         -1.1898e-02, -1.4211e-03,  2.4628e-04,  1.7840e-02, -1.6059e-02,\n",
      "          9.7899e-03,  1.1204e-03,  1.1370e-02, -8.4719e-03,  3.1243e-03,\n",
      "         -8.7562e-03,  8.6316e-03,  1.4429e-02, -9.1066e-04, -1.0439e-02,\n",
      "         -1.0606e-03,  1.2132e-02,  1.3410e-03, -1.6972e-03,  1.7487e-02,\n",
      "         -2.4755e-03,  2.1413e-03, -6.4573e-03,  4.4428e-03, -1.6160e-02,\n",
      "         -5.5894e-03, -9.4236e-03,  1.7522e-02,  1.4241e-03, -4.4016e-04,\n",
      "         -1.6316e-03,  1.1571e-02, -1.3421e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = convLayer((torch.ones((1,1,32,32))))\n",
    "\n",
    "print(a)\n",
    "\n",
    "label = torch.ones((1,128))\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(convLayer.parameters(),lr = 0.01)\n",
    "\n",
    "optimizer.zero_grad\n",
    "\n",
    "loss = lossFunction(a,label)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convLayer.boxes[0].a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
